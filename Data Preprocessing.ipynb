{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\srish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "## loading the dataset\n",
    "data = gutenberg.raw(\"blake-poems.txt\")\n",
    "\n",
    "##Saving to a file\n",
    "with open(\"blake-poems.txt\",\"w\") as file:\n",
    "    file.write(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\srish\\Downloads\\Next-Word-Prediction\\venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "1551\n"
     ]
    }
   ],
   "source": [
    "## Import necessary libraries for text preprocessing and data splitting\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # Tokenizer for converting text to numerical sequences\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences  # For padding sequences to the same length\n",
    "from sklearn.model_selection import train_test_split  # For splitting dataset into training and testing sets\n",
    "\n",
    "## Loading the dataset (Jane Austen's 'Emma')\n",
    "with open('blake-poems.txt', \"r\") as file:  # Open the text file in read mode\n",
    "    text = file.read().lower()  # Read the entire text and convert it to lowercase to standardize the data\n",
    "    \n",
    "## Tokenizing the text\n",
    "tokenizer = Tokenizer()  # Initialize the tokenizer\n",
    "tokenizer.fit_on_texts([text])  # Create a dictionary where each unique word in the text is assigned a unique integer ID\n",
    "total_words = len(tokenizer.word_index) + 1  # Get the total number of unique words (vocabulary size), adding 1 because Keras starts indexing at 1\n",
    "\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating input sequences\n",
    "input_sequences = []  # Initialize an empty list to store input sequences\n",
    "\n",
    "# Split the text into lines, and for each line, create tokenized sequences\n",
    "for line in text.split('/n'):  # Split the text at every newline ('\\n') character, treating each line as a separate sequence\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]  # Convert each line into a sequence of tokens (integers) using the tokenizer\n",
    "    # For each tokenized line, generate n-gram sequences\n",
    "    for i in range(1, len(token_list)):  # Iterate over the tokenized line, starting from the second token\n",
    "        n_gram_sequence = token_list[:i+1]  # Create an n-gram sequence: slice the token list from the beginning up to the (i+1)-th token\n",
    "        input_sequences.append(n_gram_sequence)  # Add the generated n-gram sequence to the input_sequences list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,   0, 717,  42],\n",
       "       [  0,   0,   0, ..., 717,  42, 460],\n",
       "       [  0,   0,   0, ...,  42, 460, 346],\n",
       "       ...,\n",
       "       [  0,   0, 717, ..., 249,   1, 130],\n",
       "       [  0, 717,  42, ...,   1, 130,   3],\n",
       "       [717,  42, 460, ..., 130,   3, 344]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Pad sequences\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Assuming input_sequences is a list of sequences, where each sequence is a list of integers (e.g., word indices).\n",
    "# Example: input_sequences = [[1, 2, 3], [4, 5], [6]]\n",
    "# The goal is to pad all sequences to a uniform length.\n",
    "\n",
    "# Step 1: Find the maximum sequence length among all input sequences.\n",
    "# This will determine the length to which all sequences will be padded.\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "\n",
    "# Step 2: Pad sequences.\n",
    "# 'pad_sequences' pads each sequence to the same length (max_sequence_len).\n",
    "# 'padding=\"pre\"' adds padding at the beginning of each sequence, meaning shorter sequences will be padded with zeros in front.\n",
    "# 'maxlen' is the argument that specifies the maximum length for padding.\n",
    "# The result is converted to a NumPy array for compatibility with further processing.\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# Step 3: Display the padded sequences.\n",
    "# The output will be an array where all sequences have the same length (max_sequence_len),\n",
    "# with shorter sequences padded with zeros at the beginning.\n",
    "input_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Create predictors (x) and labels (y) from the padded sequences.\n",
    "# Predictors (x) are all elements of the sequences except the last one (features).\n",
    "# Labels (y) are the last element of each sequence (targets).\n",
    "x, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "\n",
    "# Step 2: Convert labels to categorical format.\n",
    "# 'tf.keras.utils.to_categorical' converts integer labels to one-hot encoded vectors.\n",
    "# 'num_classes' specifies the total number of classes (e.g., total number of unique words).\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
    "\n",
    "# Step 3: Split the data into training and testing sets.\n",
    "# 'train_test_split' divides the data into training and testing sets based on the 'test_size' ratio.\n",
    "# Here, 20% of the data is used for testing, and the rest is used for training.\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\srish\\Downloads\\Next-Word-Prediction\\venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras.callback'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping\n\u001b[0;32m      2\u001b[0m early_stopping\u001b[38;5;241m=\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras.callback'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callback import EarlyStopping\n",
    "early_stopping=EarlyStopping(monitor='val_loss',patience=3,restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (None, 6838, 100)         155100    \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 200)               240800    \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1551)              311751    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 707651 (2.70 MB)\n",
      "Trainable params: 707651 (2.70 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:From c:\\Users\\srish\\Downloads\\Next-Word-Prediction\\venv\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\srish\\Downloads\\Next-Word-Prediction\\venv\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "  4/171 [..............................] - ETA: 3:39:20 - loss: 7.3457 - accuracy: 0.0234  "
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train,y_train,epochs=50,validation_data=(x_test,y_test),verbose=1,callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\srish\\Downloads\\Next-Word-Prediction\\venv\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 6838, 128)         198528    \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 200)               183200    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 200)               0         \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 200)               800       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               25728     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1551)              200079    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 608335 (2.32 MB)\n",
      "Trainable params: 607935 (2.32 MB)\n",
      "Non-trainable params: 400 (1.56 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules from tensorflow.keras\n",
    "\n",
    "# Sequential: Allows us to build a model layer by layer in sequence.\n",
    "# Embedding: Converts integer-encoded words into dense vectors of fixed size (word embeddings).\n",
    "# LSTM: Long Short-Term Memory layer for processing sequences.\n",
    "# Dense: Fully connected layer for making predictions.\n",
    "# Dropout: Regularization technique that randomly sets some neurons to zero during training to prevent overfitting.\n",
    "# Bidirectional: Wraps an LSTM layer to process the input sequence in both forward and backward directions.\n",
    "# BatchNormalization: Normalizes the output of the previous layer to stabilize training.\n",
    "# Adam: Optimizer that adapts the learning rate during training.\n",
    "# EarlyStopping: Stops training when the validation performance starts to degrade to prevent overfitting.\n",
    "# ReduceLROnPlateau: Reduces the learning rate when the model performance plateaus to fine-tune training.\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Initialize a Sequential model\n",
    "# Sequential: Initializes a model where you can stack layers sequentially.\n",
    "model = Sequential()\n",
    "\n",
    "# Add an Embedding layer\n",
    "# Converts words into dense vectors (embeddings).\n",
    "# total_words: Defines the size of the vocabulary (number of unique words).\n",
    "# 128: Specifies the dimension of each word vector (embedding size).\n",
    "# input_length: Defines the maximum length of input sequences (each input will have this fixed length).\n",
    "model.add(Embedding(total_words, 128, input_length=max_sequence_len-1))\n",
    "\n",
    "# Add a Bidirectional LSTM layer\n",
    "# Processes input sequences from both forward and backward directions, improving the context understanding.\n",
    "# 256: Specifies the number of LSTM units (output dimensions).\n",
    "# return_sequences=True: Ensures that the full sequence of outputs is returned, which is necessary for stacking additional LSTM layers.\n",
    "model.add(Bidirectional(LSTM(100)))\n",
    "\n",
    "# Add a Dropout layer for regularization\n",
    "# 0.2: Specifies the dropout rate, meaning 20% of the neurons will be randomly set to 0 during training to prevent overfitting and improve generalization.\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Add a Batch Normalization layer\n",
    "# Normalizes the output of the previous layer, which helps to stabilize and accelerate training by reducing the internal covariate shift.\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Add another Dropout layer for regularization\n",
    "# 0.2: Another Dropout layer with a 0.2 rate to further reduce overfitting.\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Add a Dense layer\n",
    "# 128: Fully connected layer with 128 units and ReLU activation, which helps to learn complex representations from the LSTM output.\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# Add the output Dense layer\n",
    "# total_words: Output layer with units equal to the vocabulary size, using the softmax activation function for multi-class classification.\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "# loss=\"categorical_crossentropy\": Loss function for multi-class classification, suitable for one-hot encoded labels.\n",
    "# optimizer=Adam(learning_rate=0.001): Adam optimizer with a custom learning rate.\n",
    "# metrics=[\"accuracy\"]: Specifies accuracy as the metric to evaluate the model's performance during training and testing.\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001), metrics=[\"accuracy\"])\n",
    "\n",
    "# Set up early stopping\n",
    "# Stops training when the validation loss stops improving, preventing overfitting by not over-training.\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Set up learning rate scheduler\n",
    "# Reduces the learning rate when the validation loss plateaus, helping the model fine-tune its weights when progress slows down.\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:From c:\\Users\\srish\\Downloads\\Next-Word-Prediction\\venv\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\srish\\Downloads\\Next-Word-Prediction\\venv\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "171/171 [==============================] - 1568s 9s/step - loss: 6.5222 - accuracy: 0.0580 - val_loss: 6.7060 - val_accuracy: 0.0658 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "171/171 [==============================] - 1704s 10s/step - loss: 6.0293 - accuracy: 0.0689 - val_loss: 6.9815 - val_accuracy: 0.0724 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "171/171 [==============================] - 1632s 10s/step - loss: 5.7493 - accuracy: 0.0821 - val_loss: 6.7174 - val_accuracy: 0.0863 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "171/171 [==============================] - 1609s 9s/step - loss: 5.4077 - accuracy: 0.1015 - val_loss: 6.4453 - val_accuracy: 0.0892 - lr: 5.0000e-04\n",
      "Epoch 5/20\n",
      "171/171 [==============================] - 1625s 10s/step - loss: 5.1786 - accuracy: 0.1186 - val_loss: 6.5351 - val_accuracy: 0.0899 - lr: 5.0000e-04\n",
      "Epoch 6/20\n",
      "171/171 [==============================] - 1627s 10s/step - loss: 4.9563 - accuracy: 0.1377 - val_loss: 6.7107 - val_accuracy: 0.0870 - lr: 5.0000e-04\n",
      "Epoch 7/20\n",
      "171/171 [==============================] - 1601s 9s/step - loss: 4.6908 - accuracy: 0.1634 - val_loss: 6.8327 - val_accuracy: 0.0906 - lr: 2.5000e-04\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 6838, 128)         198528    \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 200)               183200    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 200)               0         \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 200)               800       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               25728     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1551)              200079    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 608335 (2.32 MB)\n",
      "Trainable params: 607935 (2.32 MB)\n",
      "Non-trainable params: 400 (1.56 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "# epochs=20: The model will train for up to 20 epochs, but early stopping may halt it sooner.\n",
    "# validation_data=(X_val, y_val): Uses a validation set to monitor model performance during training.\n",
    "# callbacks=[early_stopping, lr_scheduler]: Applies the early stopping and learning rate reduction callbacks during training.\n",
    "history = model.fit(x_train, y_train, epochs=20, validation_data=(x_test, y_test), callbacks=[early_stopping, lr_scheduler])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "171/171 [==============================] - 1499s 9s/step - loss: 4.5443 - accuracy: 0.1761 - val_loss: 6.9414 - val_accuracy: 0.0885 - lr: 2.5000e-04\n",
      "Epoch 2/100\n",
      "171/171 [==============================] - 1654s 10s/step - loss: 4.4082 - accuracy: 0.1885 - val_loss: 7.0530 - val_accuracy: 0.0841 - lr: 2.5000e-04\n",
      "Epoch 3/100\n",
      "171/171 [==============================] - 1606s 9s/step - loss: 4.2546 - accuracy: 0.2137 - val_loss: 7.1945 - val_accuracy: 0.0848 - lr: 2.5000e-04\n",
      "Epoch 4/100\n",
      "171/171 [==============================] - 1602s 9s/step - loss: 4.0992 - accuracy: 0.2289 - val_loss: 7.3064 - val_accuracy: 0.0863 - lr: 1.2500e-04\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test), callbacks=[early_stopping, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the next word\n",
    "def predict_next_word(model, tokenizer, text, max_sequence_len):\n",
    "    # Tokenize the input text to convert it into a sequence of integers\n",
    "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "    \n",
    "    # If the sequence is longer than or equal to max_sequence_len, truncate it to the last max_sequence_len-1 tokens\n",
    "    if len(token_list) >= max_sequence_len:\n",
    "        token_list = token_list[-(max_sequence_len-1):]  # Ensure the sequence length matches max_sequence_len-1\n",
    "    \n",
    "    # Pad the sequence to ensure it's of length max_sequence_len-1\n",
    "    # Padding is applied at the beginning ('pre') if the sequence is shorter\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    \n",
    "    # Use the model to predict the next word, based on the tokenized input\n",
    "    predicted = model.predict(token_list, verbose=0)\n",
    "    \n",
    "    # Get the index of the predicted word by finding the highest probability in the model's output\n",
    "    predicted_word_index = np.argmax(predicted, axis=1)\n",
    "    \n",
    "    # Loop through the tokenizer's word index to find the word corresponding to the predicted index\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted_word_index:\n",
    "            return word  # Return the predicted word\n",
    "    \n",
    "    # If no word is found (which is rare), return None\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: Summer breeze\n",
      "Next Word Prediction: by\n"
     ]
    }
   ],
   "source": [
    "# Function to predict the next word\n",
    "def predict_next_word(model, tokenizer, text, max_sequence_len):\n",
    "    # Tokenize the input text to convert it into a sequence of integers\n",
    "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "    \n",
    "    # If the sequence is longer than or equal to max_sequence_len, truncate it to the last max_sequence_len-1 tokens\n",
    "    if len(token_list) >= max_sequence_len:\n",
    "        token_list = token_list[-(max_sequence_len-1):]  # Ensure the sequence length matches max_sequence_len-1\n",
    "    \n",
    "    # Pad the sequence to ensure it's of length max_sequence_len-1\n",
    "    # Padding is applied at the beginning ('pre') if the sequence is shorter\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    \n",
    "    # Use the model to predict the next word, based on the tokenized input\n",
    "    predicted = model.predict(token_list, verbose=0)\n",
    "    \n",
    "    # Get the index of the predicted word by finding the highest probability in the model's output\n",
    "    predicted_word_index = np.argmax(predicted, axis=1)\n",
    "    \n",
    "    # Loop through the tokenizer's word index to find the word corresponding to the predicted index\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted_word_index:\n",
    "            return word  # Return the predicted word\n",
    "    \n",
    "    # If no word is found (which is rare), return None\n",
    "    return None\n",
    "\n",
    "# Example usage\n",
    "input_text = \"Summer breeze\"\n",
    "print(f\"Input text: {input_text}\")\n",
    "\n",
    "# The maximum sequence length is determined based on the model's input shape\n",
    "max_sequence_len = model.input_shape[1] + 1  # Adding 1 because model input is typically one less than sequence length\n",
    "\n",
    "# Predict the next word\n",
    "next_word = predict_next_word(model, tokenizer, input_text, max_sequence_len)\n",
    "print(f\"Next Word Prediction: {next_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\srish\\Downloads\\Next-Word-Prediction\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model.save(\"next_word_lstm.h5\")  # The model is saved in HDF5 format with the filename \"next_word_lstm.h5\"\n",
    "\n",
    "# Save the tokenizer using pickle\n",
    "import pickle\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    # The tokenizer is saved using pickle for later use\n",
    "    # The highest protocol ensures efficient saving and loading\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:   On the echoing\n",
      "Next Word Prediction: night\n"
     ]
    }
   ],
   "source": [
    "# Input text for prediction\n",
    "input_text = \"  On the echoing\"\n",
    "print(f\"Input text: {input_text}\")\n",
    "\n",
    "# Determine the maximum sequence length from the model's input shape\n",
    "max_sequence_len = model.input_shape[1] + 1  # Adding 1 to match sequence length for prediction\n",
    "\n",
    "# Predict the next word based on the input text\n",
    "next_word = predict_next_word(model, tokenizer, input_text, max_sequence_len)\n",
    "\n",
    "# Print the predicted next word\n",
    "print(f\"Next Word Prediction: {next_word}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
